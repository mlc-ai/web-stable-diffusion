{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Stable Diffusion - TVMCon 2023\n",
    "\n",
    "https://mlc.ai/web-stable-diffusion\n",
    "\n",
    "This project brings stable diffusion models to web browsers. **Everything runs inside the browser with no server support.** To our knowledge, this is the the world’s first stable diffusion completely running on the browser. Now let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![workflow](../site/img/fig/workflow.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages\n",
    "\n",
    "To import and build the model, we first need to install the on-going development of TVM Unity and other dependencies with the following pip command.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --pre torch --upgrade --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "!python3 -m pip install diffusers transformers accelerate\n",
    "!python3 -m pip install mlc-ai-nightly -f https://mlc.ai/wheels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import necessary packages and set up the artifact directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.script import relax as R\n",
    "\n",
    "import torch\n",
    "from torch import fx\n",
    "\n",
    "from web_stable_diffusion import trace\n",
    "from web_stable_diffusion import utils\n",
    "\n",
    "torch_dev_key = \"mps\"\n",
    "target = tvm.target.Target(\n",
    "    \"webgpu\", host=\"llvm -mtriple=wasm32-unknown-unknown-wasm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import stable diffusion models\n",
    "\n",
    "With necessary packages imported, the first step is to import the stable diffusion PyTorch models into TVM. Here we also leverage the techniques and interfaces about [TorchDynamo](https://pytorch.org/tutorials/intermediate/dynamo_tutorial.html) and [Torch FX](https://pytorch.org/docs/stable/fx.html) introduced before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](../site/img/fig/pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with building and deploying the stable diffusion model to local CUDA backend, building and deploying the model to web with WebGPU runtime is special, mostly because of the difference of runtime environment:\n",
    "\n",
    "**Web runtime has no Python (let alone PyTorch, NumPy). We can only leverage JavaScript for model deployment on web.**\n",
    "\n",
    "Considering this major difference, we will need to _simplify the runtime stable diffusion pipeline (written in JavaScript) as much as possible_, and maximize the number of tasks in the build stage, ahead of the web model deployment. Our workflow demonstrates this principal in the two following aspects:\n",
    "1. capture more computation to TVM’s IRModule,\n",
    "2. separate models’ weights from the IRModule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Capture more computation to TVM’s IRModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous CUDA deployment where we only import the CLIP text encoder, the UNet and the VAE decoder to Relax and leave all other steps as PyTorch operations.\n",
    "\n",
    "However, in web runtime we do not have PyTorch. As result, it is necessary to cover those additional operations in our IRModule as well. There are two different approaches to the same goal. Both are very simple:\n",
    "1. implement the operations manually with existing Relax infrastructure, or\n",
    "2. write a wrapper `torch.nn.Module` which both contains the ML model and the appending/prepending operations.\n",
    "\n",
    "In our practice, we adopt both approaches. For some operations we use wrapper `nn.Module`, while for others we write the operation manually. Let’s walk through each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ①. The CLIP text encoder\n",
    "\n",
    "In the entire pipeline, the text encoder is used twice: one for the prompt and the other one for the negative prompt. Since it is next to the tokenization (which we do not import) and is followed by the concatenation of both prompts’ embeddings, the encoder is a standalone phase.\n",
    "\n",
    "Therefore, we use an `nn.Module` to wrap the single CLIP forward, and use `dynamo_capture_subgraphs` to import the `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.relax.frontend.torch import dynamo_capture_subgraphs\n",
    "from tvm.relax.frontend.torch import from_fx\n",
    "\n",
    "def clip_to_text_embeddings(pipe) -> tvm.IRModule:\n",
    "    # Define the wrapper torch.nn.Module for CLIP.\n",
    "    class CLIPModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, clip):\n",
    "            super().__init__()\n",
    "            self.clip = clip\n",
    "\n",
    "        def forward(self, text_input_ids):\n",
    "            text_embeddings = self.clip(text_input_ids)[0]\n",
    "            return text_embeddings\n",
    "\n",
    "    clip = pipe.text_encoder\n",
    "    clip_to_text_embeddings = CLIPModelWrapper(clip)\n",
    "\n",
    "    # Create random input (77 is the maximum length).\n",
    "    text_input_ids = torch.rand((1, 77)).to(torch.int32)\n",
    "    # Capture CLIP's computational graph.\n",
    "    mod = dynamo_capture_subgraphs(\n",
    "        clip_to_text_embeddings.forward,\n",
    "        text_input_ids,\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    assert len(mod.functions) == 1\n",
    "\n",
    "    return tvm.IRModule({\"clip\": mod[\"subgraph_0\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ②. The embedding concatenation\n",
    "\n",
    "This stage concatenates the embeddings of both prompts, and is followed by the huge UNet iteration. It is also standalone, and here we choose to implement the concatenation by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_embeddings() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "    cond_embeddings = relax.Var(\"cond_embeddings\", R.Tensor([1, 77, 768], \"float32\"))\n",
    "    uncond_embeddings = relax.Var(\n",
    "        \"uncond_embeddings\", R.Tensor([1, 77, 768], \"float32\")\n",
    "    )\n",
    "    with bb.function(\"concat_embeddings\", [cond_embeddings, uncond_embeddings]):\n",
    "        res = bb.emit(\n",
    "            relax.op.concat([cond_embeddings, uncond_embeddings], axis=0)\n",
    "        )\n",
    "        bb.emit_func_output(res)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ③. Latent concat + UNet + classifier-free guidance\n",
    "\n",
    "The third stage is the first part of the UNet loop body. It is mostly the UNet forward, while before the UNet forward there is a step of latent concatenation, and after UNet forward, one step of [classifier-free guidance](https://github.com/huggingface/diffusers/blob/79eb3d07d07a2dada172c5958d6fca478c860f16/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L674-L677) will be performed to force the generation to better match the prompt potentially. Since the latent concatenation and the guidance are immediately before/after the UNet forward which we always have to import whatever, we use a wrapper `nn.Module` to import all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_latents_to_noise_pred(pipe, device_str: str) -> tvm.IRModule:\n",
    "    class UNetModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, unet):\n",
    "            super().__init__()\n",
    "            self.unet = unet\n",
    "            # Default guidance scale factor in stable diffusion.\n",
    "            self.guidance_scale = 7.5\n",
    "\n",
    "        def forward(self, latents, timestep_tensor, text_embeddings):\n",
    "            # Latent concatenation.\n",
    "            latent_model_input = torch.cat([latents] * 2, dim=0)\n",
    "            # UNet forward.\n",
    "            noise_pred = self.unet(latent_model_input, timestep_tensor, text_embeddings)\n",
    "            # Classifier-free guidance.\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + self.guidance_scale * (\n",
    "                noise_pred_text - noise_pred_uncond\n",
    "            )\n",
    "            return noise_pred\n",
    "\n",
    "    unet = utils.get_unet(pipe, device_str)\n",
    "    unet_to_noise_pred = UNetModelWrapper(unet)\n",
    "    graph = fx.symbolic_trace(unet_to_noise_pred)\n",
    "    mod = from_fx(\n",
    "        graph,\n",
    "        [((1, 4, 64, 64), \"float32\"), ((), \"int32\"), ((2, 77, 768), \"float32\")],\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    return tvm.IRModule({\"unet\": mod[\"main\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ④. Scheduler step\n",
    "\n",
    "The scheduler step stage is the other part of the UNet iteration, and is very important for updating the latents towards the denoising direction. There are many kinds of different schedulers, with each having (possibly) largely different implementation. Here we use the multi-step DPM solver scheduler.\n",
    "\n",
    "One feature of schedulers is that schedulers usually maintain a list of history UNet output, and the scheduler step operation takes the maintained history as input internally. Since the step operation is history dependent, we are not able to combine the scheduler step together with the previous UNet part, and have to implement it separately. Because the scheduler step is also standalone mostly, we implement it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpm_solver_multistep_scheduler_steps() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    # convert_model_output, the first function in multi-step DPM solver.\n",
    "    sample = relax.Var(\"sample\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    model_output = relax.Var(\"model_output\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    alpha = relax.Var(f\"alpha\", R.Tensor((), \"float32\"))\n",
    "    sigma = relax.Var(f\"sigma\", R.Tensor((), \"float32\"))\n",
    "    with bb.function(\n",
    "        \"dpm_solver_multistep_scheduler_convert_model_output\",\n",
    "        [sample, model_output, alpha, sigma],\n",
    "    ):\n",
    "        converted_model_output = bb.emit(\n",
    "            (sample - sigma * model_output) / alpha, \"converted_model_output\"\n",
    "        )\n",
    "        bb.emit_func_output(converted_model_output)\n",
    "\n",
    "    # step, the second function.\n",
    "    sample = relax.Var(\"sample\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    model_output = relax.Var(\"model_output\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    last_model_output = relax.Var(\n",
    "        \"last_model_output\", R.Tensor((1, 4, 64, 64), \"float32\")\n",
    "    )\n",
    "    consts = [relax.Var(f\"c{i}\", R.Tensor((), \"float32\")) for i in range(3)]\n",
    "\n",
    "    with bb.function(\n",
    "        \"dpm_solver_multistep_scheduler_step\",\n",
    "        [sample, model_output, last_model_output, *consts],\n",
    "    ):\n",
    "        prev_sample = bb.emit(\n",
    "            consts[0] * sample\n",
    "            - consts[1] * model_output\n",
    "            - consts[2] * (model_output - last_model_output),\n",
    "            \"prev_sample\",\n",
    "        )\n",
    "        bb.emit_func_output(prev_sample)\n",
    "\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⑤. VAE + image normalization\n",
    "\n",
    "The last but one stage is the VAE step followed by an image normalization, which normalizes the value range from `[-1, 1]` to integers in `[0, 255]`. For the same reason as ③, we use a wrapper `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_to_image(pipe) -> tvm.IRModule:\n",
    "    class VAEModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, vae):\n",
    "            super().__init__()\n",
    "            self.vae = vae\n",
    "\n",
    "        def forward(self, latents):\n",
    "            # Scale the latents so that it can be decoded by VAE.\n",
    "            latents = 1 / 0.18215 * latents\n",
    "            # VAE decode\n",
    "            z = self.vae.post_quant_conv(latents)\n",
    "            image = self.vae.decoder(z)\n",
    "            # Image normalization\n",
    "            image = (image / 2 + 0.5).clamp(min=0, max=1)\n",
    "            image = (image.permute(0, 2, 3, 1) * 255).round()\n",
    "            return image\n",
    "\n",
    "    vae = pipe.vae\n",
    "    vae_to_image = VAEModelWrapper(vae)\n",
    "\n",
    "    z = torch.rand((1, 4, 64, 64), dtype=torch.float32)\n",
    "    mod = dynamo_capture_subgraphs(\n",
    "        vae_to_image.forward,\n",
    "        z,\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    assert len(mod.functions) == 1\n",
    "\n",
    "    return tvm.IRModule({\"vae\": mod[\"subgraph_0\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⑥. Image conversion to RGBA\n",
    "\n",
    "To display the image, we need to convert the image to RGBA mode that can be directly rendered by the web runtime. This conversion requires dtype `uint32`, which PyTorch doesn’t support. Therefore, we are unable to combine this stage with the previous one, and need to implement it by hand with Relax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_rgba() -> tvm.IRModule:\n",
    "    from tvm import te\n",
    "\n",
    "    def f_image_to_rgba(A):\n",
    "        def fcompute(y, x):\n",
    "            return (\n",
    "                A[0, y, x, 0].astype(\"uint32\")\n",
    "                | (A[0, y, x, 1].astype(\"uint32\") << 8)\n",
    "                | (A[0, y, x, 2].astype(\"uint32\") << 16)\n",
    "                | tvm.tir.const(255 << 24, \"uint32\")\n",
    "            )\n",
    "\n",
    "        return te.compute((512, 512), fcompute, name=\"image_to_rgba\")\n",
    "\n",
    "    bb = relax.BlockBuilder()\n",
    "    x = relax.Var(\"x\", R.Tensor([1, 512, 512, 3], \"float32\"))\n",
    "    with bb.function(\"image_to_rgba\", [x]):\n",
    "        image = bb.emit(\n",
    "            bb.call_te(f_image_to_rgba, x, primfunc_name_hint=\"tir_image_to_rgba\")\n",
    "        )\n",
    "        bb.emit_func_output(image)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine every piece together\n",
    "\n",
    "We have described how we import every part of the stable diffusion pipeline into Relax. Now we can combine every of them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7331cabf37184cf096c4c9c9c4ef2241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruihang-macstudio/Workspace/miniforge3/envs/python310/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "clip = clip_to_text_embeddings(pipe)\n",
    "unet = unet_latents_to_noise_pred(pipe, torch_dev_key)\n",
    "vae = vae_to_image(pipe)\n",
    "concat_embeddings = concat_embeddings()\n",
    "image_to_rgba = image_to_rgba()\n",
    "schedulers = [dpm_solver_multistep_scheduler_steps()]\n",
    "\n",
    "mod: tvm.IRModule = utils.merge_irmodules(\n",
    "    clip,\n",
    "    unet,\n",
    "    vae,\n",
    "    concat_embeddings,\n",
    "    image_to_rgba,\n",
    "    *schedulers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Separate models’ weights from the IRModule\n",
    "\n",
    "To reduce the size of the built artifact so that it can be universally deployed everywhere (including the web), we separate models’ weights from the IRModule we get. For a weight tensor, we use a placeholder to represent it in the IRModule, instead of letting it reside in the IRModule as a constant tensor. We will save the separated weights to the disk later. At the beginning of the deployment, we will load these weights from disk to memory.\n",
    "\n",
    "The separation is implemented as function `relax.frontend.detach_params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, params = relax.frontend.detach_params(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to print out the entire IRModule via \n",
    "```python\n",
    "mod.show()\n",
    "```\n",
    "to see the models and other functions we have imported in the IRModule. The output will be thousands of lines long, so we do not run it live here. If you try it out, the printed output should look in the following way:\n",
    "```python\n",
    "# from tvm.script import ir as I\n",
    "# from tvm.script import relax as R\n",
    "\n",
    "@I.ir_module\n",
    "class Module:\n",
    "    @R.function\n",
    "    def clip(\n",
    "        inp_0: R.Tensor((1, 77), dtype=\"int32\"),\n",
    "        self_clip_text_model_embeddings_position_embedding_weight: R.Tensor((77, 768), dtype=\"float32\"),\n",
    "        self_clip_text_model_embeddings_token_embedding_weight: R.Tensor((49408, 768), dtype=\"float32\"),\n",
    "        ...\n",
    "    ) -> R.Tensor((1, 77, 768), dtype=\"float32\"):\n",
    "        R.func_attr({\"num_input\": 1})\n",
    "        with R.dataflow():\n",
    "            lv: R.Tensor((1, 77), dtype=\"int32\") = R.reshape(inp_0, R.shape([1, 77]))\n",
    "            lv1: R.Tensor((1, 77), dtype=\"int32\") = R.astype(lv, dtype=\"int32\")\n",
    "            lv2: R.Tensor((77,), dtype=\"int32\") = R.reshape(lv1, R.shape([77]))\n",
    "            lv3: R.Tensor((77, 768), dtype=\"float32\") = R.take(self_clip_text_model_embeddings_token_embedding_weight, lv2, axis=0)\n",
    "            lv4: R.Tensor((1, 77, 768), dtype=\"float32\") = R.reshape(lv3, R.shape([1, 77, 768]))\n",
    "            lv5: R.Tensor((1, 77), dtype=\"int32\") = R.astype(metadata[\"relax.expr.Constant\"][0], dtype=\"int32\")\n",
    "            lv6: R.Tensor((77,), dtype=\"int32\") = R.reshape(lv5, R.shape([77]))\n",
    "            lv7: R.Tensor((77, 768), dtype=\"float32\") = R.take(self_clip_text_model_embeddings_position_embedding_weight, lv6, axis=0)\n",
    "            lv8: R.Tensor((1, 77, 768), dtype=\"float32\") = R.reshape(lv7, R.shape([1, 77, 768]))\n",
    "            lv9: R.Tensor((1, 77, 768), dtype=\"float32\") = R.add(lv4, lv8)\n",
    "            ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of printing out the whole IRModule, what we can do is to print out the names of the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpm_solver_multistep_scheduler_step\n",
      "dpm_solver_multistep_scheduler_convert_model_output\n",
      "image_to_rgba\n",
      "clip\n",
      "unet\n",
      "concat_embeddings\n",
      "vae\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_relax_funcnames(mod: tvm.IRModule):\n",
    "    for global_var, func in mod.functions.items():\n",
    "        if isinstance(func, relax.Function):\n",
    "            print(global_var.name_hint)\n",
    "    print()\n",
    "\n",
    "\n",
    "print_relax_funcnames(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out one of the weight tensors to see what we have captured for model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tvm.nd.NDArray shape=(77, 768), cpu(0)>\n",
       "array([[ 0.00158362,  0.0020091 ,  0.00020799, ..., -0.00130294,\n",
       "         0.0007798 ,  0.00150727],\n",
       "       [ 0.00423452,  0.00287621,  0.00020198, ...,  0.00103357,\n",
       "         0.0014911 , -0.00119652],\n",
       "       [ 0.00183514,  0.00073841, -0.00124233, ..., -0.00294402,\n",
       "        -0.00091987,  0.00255763],\n",
       "       ...,\n",
       "       [ 0.02157524,  0.00553936, -0.01014109, ..., -0.00649147,\n",
       "        -0.00294858,  0.00372774],\n",
       "       [ 0.0188203 ,  0.00729219, -0.00766407, ..., -0.00251736,\n",
       "        -0.00087413,  0.00567614],\n",
       "       [ 0.03300093,  0.02810323,  0.0288674 , ...,  0.01597873,\n",
       "         0.01021753, -0.03095413]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first weight parameter of the CLIP model.\n",
    "params[\"clip\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we have went through all steps of importing the stable diffusion model to Relax for web deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize and bulild the model\n",
    "\n",
    "This section talks about how we optimize the stable diffusion model in TVM Unity, and how we build it to the WebGPU backend. We will very briefly go through these steps as they are not the focus of this tutorial. If you are interested, we have a walkthrough notebook which focuses more on the optimization and build at https://github.com/mlc-ai/web-stable-diffusion/blob/main/walkthrough.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "The optimization for the model is mainly kernel fusion and constant folding.\n",
    "\n",
    "We also apply the pre-tuned MetaSchedule database to the IRModule, so that each operator in the IRModule is well-optimized and ready to be built to GPU backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:32:51] /Users/ruihang-macstudio/Workspace/tvm/include/tvm/topi/transform.h:1080: Warning: Fast mode segfaults when there are out-of-bounds indices. Make sure input indices are in bound\n",
      "[12:32:51] /Users/ruihang-macstudio/Workspace/tvm/include/tvm/topi/transform.h:1080: Warning: Fast mode segfaults when there are out-of-bounds indices. Make sure input indices are in bound\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"clip\", \"unet\", \"vae\"]\n",
    "scheduler_func_names = [\n",
    "    \"dpm_solver_multistep_scheduler_convert_model_output\",\n",
    "    \"dpm_solver_multistep_scheduler_step\",\n",
    "]\n",
    "entry_funcs = (\n",
    "    model_names + scheduler_func_names + [\"image_to_rgba\", \"concat_embeddings\"]\n",
    ")\n",
    "\n",
    "# The default Relax pipeline contains kernel fusion and constant folding.\n",
    "mod = relax.pipeline.get_pipeline()(mod)\n",
    "\n",
    "# Some other transformations.\n",
    "mod = relax.transform.RemoveUnusedFunctions(entry_funcs)(mod)\n",
    "mod = relax.transform.LiftTransformParams()(mod)\n",
    "mod_transform, mod_deploy = utils.split_transform_deploy_mod(\n",
    "    mod, model_names, entry_funcs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MetaSchedule database.\n",
    "from tvm import meta_schedule as ms\n",
    "\n",
    "db = ms.database.create(work_dir=\"log_db\")\n",
    "with target, db, tvm.transform.PassContext(opt_level=3):\n",
    "    mod_deploy = relax.transform.MetaScheduleApplyDatabase()(mod_deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for build\n",
    "\n",
    "As previously mentioned, we need to save the models’ weights to the disk. In addition to the weights, we also store the constants used by the scheduler at each step of iteration to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start storing to cache dist/params\n",
      "[1054/1054] saving clip_195\n",
      "All finished, 61 total shards committed, record saved to dist/params/ndarray-cache.json\n",
      "Also saved a bf16 record to dist/params/ndarray-cache-b16.json\n"
     ]
    }
   ],
   "source": [
    "trace.compute_save_scheduler_consts(artifact_path=\"dist\")\n",
    "new_params = utils.transform_params(mod_transform, params)\n",
    "utils.save_params(new_params, artifact_path=\"dist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build\n",
    "\n",
    "We build the model to WebGPU backend by `relax.build`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:33:13] /Users/ruihang-macstudio/Workspace/tvm/src/target/llvm/codegen_llvm.cc:177: Warning: Set native vector bits to be 128 for wasm32\n"
     ]
    }
   ],
   "source": [
    "ex = relax.build(mod_deploy, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then export the build artifact to the disk, which we can load back in web runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.export_library(\"dist/stable_diffusion_webgpu.wasm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on web\n",
    "\n",
    "As the last step, let’s try to deploy the stable diffusion to web end with WebGPU runtime.\n",
    "* We have implemented the stable diffusion pipeline as described before [in JavaScript](https://github.com/mlc-ai/web-stable-diffusion/blob/main/web/stable_diffusion.js) in ahead. It connects everything together and has about 500 lines of code.\n",
    "* To deploy the stable diffusion to web, we actually need to run a shell script before build. Here we assume we have run the script, and just set up the site and show the final demo.\n",
    "\n",
    "For detailed instructions, please refer to our GitHub repo https://github.com/mlc-ai/web-stable-diffusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the website\n",
    "\n",
    "The last thing to do is setting up the site by running the following command in a terminal session:\n",
    "```shell\n",
    "./scripts/local_deploy_site.sh\n",
    "```\n",
    "\n",
    "Once the website is set up, open `localhost:8888/web-stable-diffusion/` in Chrome Canary to try out the demo on your local machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ scripts/build_site.sh web/local-config.json\n",
      "+ [[ ! -f web/local-config.json ]]\n",
      "+ rm -rf site/dist\n",
      "+ mkdir -p site/dist site/_inlcudes\n",
      "+ echo 'Copy local configurations..'\n",
      "Copy local configurations..\n",
      "+ cp web/local-config.json site/stable-diffusion-config.json\n",
      "+ echo 'Copy files...'\n",
      "Copy files...\n",
      "+ cp web/stable_diffusion.html site/_includes\n",
      "+ cp web/stable_diffusion.js site/dist\n",
      "+ cp dist/scheduler_pndm_consts.json site/dist\n",
      "+ cp dist/scheduler_dpm_solver_multistep_consts.json site/dist\n",
      "+ cp dist/stable_diffusion_webgpu.wasm site/dist\n",
      "+ cp dist/tvmjs_runtime.wasi.js site/dist\n",
      "+ cp dist/tvmjs.bundle.js site/dist\n",
      "+ cp -r dist/tokenizers-wasm site/dist\n",
      "+ cd site\n",
      "+ jekyll b\n",
      "Configuration file: /Users/ruihang-macstudio/Workspace/web-stable-diffusion/site/_config.yml\n",
      "            Source: /Users/ruihang-macstudio/Workspace/web-stable-diffusion/site\n",
      "       Destination: /Users/ruihang-macstudio/Workspace/web-stable-diffusion/site/_site\n",
      " Incremental build: disabled. Enable with --incremental\n",
      "      Generating... \n",
      "      Remote Theme: Using theme mlc-ai/jekyll-theme-mlc\n",
      "                    done in 0.425 seconds.\n",
      " Auto-regeneration: disabled. Use --watch to enable.\n",
      "+ cd ..\n",
      "+ echo 'symlink parameter location to site..'\n",
      "symlink parameter location to site..\n",
      "++ pwd\n",
      "+ ln -s /Users/ruihang-macstudio/Workspace/web-stable-diffusion/dist/params site/_site/web-sd-shards-v1-5\n",
      "+ cd site\n",
      "+ jekyll serve --skip-initial-build --host localhost --baseurl /web-stable-diffusion --port 8888\n",
      "Configuration file: /Users/ruihang-macstudio/Workspace/web-stable-diffusion/site/_config.yml\n",
      "\u001b[33m     Build Warning: Skipping the initial build. This may result in an out-of-date site.\u001b[0m\n",
      " Auto-regeneration: enabled for '/Users/ruihang-macstudio/Workspace/web-stable-diffusion/site'\n",
      "    Server address: http://localhost:8888/web-stable-diffusion/\n",
      "  Server running... press ctrl-c to stop.\n"
     ]
    }
   ],
   "source": [
    "!./scripts/local_deploy_site.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
